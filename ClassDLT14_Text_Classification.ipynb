{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MMmcGqLxPIoo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cq5LmV6PIo7"
      },
      "source": [
        "## HugginFace's Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AbW7Cd8jPIo8"
      },
      "outputs": [],
      "source": [
        "import tqdm as notebook_tqdm\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhxCrkJjPIo8",
        "outputId": "b2ea14f9-f072-4a88-af29-9d384a40c240"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['follow', 'the', 'white', 'rabbit', 'neo']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "new_sentence = 'follow the white rabbit neo'\n",
        "new_tokens = tokenizer.tokenize(new_sentence)\n",
        "new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6DlCKGxPIo8",
        "outputId": "b3d80a24-5d77-4b45-f8bb-3f7e6cdf41e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3582, 1996, 2317, 10442, 9253]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "new_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
        "new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06gmmhDvPIo9",
        "outputId": "ce09afaa-d595-4f8d-f380-151c7517584b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 3582, 1996, 2317, 10442, 9253, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "new_ids = tokenizer.encode(new_sentence)\n",
        "new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98Ccyt_LPIo9",
        "outputId": "7fbc85d8-24d8-4e5a-df87-92462bc5d0a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'follow', 'the', 'white', 'rabbit', 'neo', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(new_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVOy3y3SPIo9",
        "outputId": "a2fc31bd-f6e8-4474-a5f0-273e0b345be9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3582, 1996, 2317, 10442, 9253]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "tokenizer.encode(new_sentence, add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGBDu8aoPIo9",
        "outputId": "c0f2e498-21ce-4f7f-ae65-de82075ccc83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 3582,  1996,  2317, 10442,  9253]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "tokenizer(new_sentence, add_special_tokens=False, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB8-t1puPIo_",
        "outputId": "f23ca022-9fc7-4432-b281-759a4a146b93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 3582, 1996, 2317, 10442, 9253, 102, 2053, 2028, 2064, 2022, 2409, 2054, 1996, 8185, 2003, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "sentence1 = 'follow the white rabbit neo'\n",
        "sentence2 = 'no one can be told what the matrix is'\n",
        "joined_sentences = tokenizer(sentence1, sentence2)\n",
        "joined_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjJ5EQqfPIo_",
        "outputId": "5ff3a4ca-e70d-4167-a3d2-5694cad58e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'follow', 'the', 'white', 'rabbit', 'neo', '[SEP]', 'no', 'one', 'can', 'be', 'told', 'what', 'the', 'matrix', 'is', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(joined_sentences['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNHKY9CrPIo_",
        "outputId": "cef1a3b1-8983-42d6-b34e-d605abeb3417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 3582, 1996, 2317, 10442, 9253, 102, 0, 0, 0, 0], [101, 2053, 2028, 2064, 2022, 2409, 2054, 1996, 8185, 2003, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "separate_sentences = tokenizer([sentence1, sentence2], padding=True)\n",
        "separate_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOQDFqKbPIpF",
        "outputId": "96400bcb-5e62-4809-d09c-274630e76b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'follow', 'the', 'white', 'rabbit', 'neo', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(separate_sentences['input_ids'][0]))\n",
        "print(separate_sentences['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqsq7GS3PIpG",
        "outputId": "8d111863-b464-43f8-9143-5e5c7df6abc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'follow', 'the', 'white', 'rabbit', 'neo', '[SEP]', 'no', 'one', 'can', 'be', 'told', 'what', 'the', 'matrix', 'is', '[SEP]']\n",
            "['[CLS]', 'another', 'first', 'sentence', '[SEP]', 'a', 'second', 'sentence', 'here', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "first_sentences = [sentence1, 'another first sentence']\n",
        "second_sentences = [sentence2, 'a second sentence here']\n",
        "batch_of_pairs = tokenizer(first_sentences, second_sentences)\n",
        "first_input = tokenizer.convert_ids_to_tokens(batch_of_pairs['input_ids'][0])\n",
        "second_input = tokenizer.convert_ids_to_tokens(batch_of_pairs['input_ids'][1])\n",
        "print(first_input)\n",
        "print(second_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DQG_yBYhPIp6"
      },
      "outputs": [],
      "source": [
        "def load_imdb_data(data_file):\n",
        "    df = pd.read_csv(data_file)\n",
        "    texts = df['review'].tolist()\n",
        "    labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
        "    return texts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1QSPsKi3SkW"
      },
      "source": [
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download&select=IMDB+Dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTJYexaz3SkW",
        "outputId": "b6092981-ba6c-4b44-eb53-ff0784889b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Files in extracted folder: ['IMDB Dataset.csv']\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile  # ✅ Import zipfile\n",
        "\n",
        "\n",
        "# Install Kaggle API if not installed\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(\"imdb-dataset-of-50k-movie-reviews.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"imdb_data\")\n",
        "\n",
        "# ✅ Unzip the downloaded file\n",
        "with zipfile.ZipFile(\"imdb-dataset-of-50k-movie-reviews.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"imdb_data\")\n",
        "\n",
        "# ✅ Verify extraction\n",
        "print(\"Files in extracted folder:\", os.listdir(\"imdb_data\"))\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset in Pandas\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"imdb_data/IMDB Dataset.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8k6M9CvV3SkX"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1YoUOcZO3SkY"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Am6sUj5C3SkZ"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zvUFpNZy3SkZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "n08Rk5zp3Ska"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "    return \"positive\" if preds.item() == 1 else \"negative\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fDc422DR3Ska"
      },
      "outputs": [],
      "source": [
        "# Set up parameters\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 2\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_epochs = 4\n",
        "learning_rate = 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "YY6AoqQi3Skb"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(review, sentiment, test_size=0.2, random_state=42)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract the text (reviews) and labels (sentiment)\n",
        "texts = df[\"review\"].tolist()  # ✅ Extract review column\n",
        "labels = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0}).tolist()  # ✅ Convert sentiment to numerical labels\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Yj7rDCdB3Skb"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer  # ✅ Import BertTokenizer\n",
        "\n",
        "# Define the model name (e.g., 'bert-base-uncased')\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# ✅ Define the dataset class\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "\n",
        "# ✅ Set a value for max_length\n",
        "max_length = 128  # You can change this value based on your model\n",
        "\n",
        "# ✅ Now this will work!\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Now this will work!\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "from torch.utils.data import DataLoader  # ✅ Import DataLoader\n",
        "\n",
        "# ✅ Define batch size (if not already defined)\n",
        "batch_size = 16  # You can change this based on your system's memory\n",
        "\n",
        "# ✅ Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "IxMEEQUQ3Skc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# ✅ Define BERT Classifier\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Extract the pooled output\n",
        "        x = self.dropout(pooled_output)\n",
        "        x = self.fc(x)  # Final classification layer\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "import torch\n",
        "from torch.optim import AdamW  # ✅ Use PyTorch's AdamW (instead of deprecated transformers version)\n",
        "from transformers import get_scheduler  # ✅ Import scheduler function\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# ✅ Define training parameters\n",
        "num_epochs = 3  # Adjust as needed\n",
        "learning_rate = 2e-5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
        "\n",
        "# ✅ Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ✅ Define learning rate scheduler\n",
        "total_steps = len(train_dataloader) * num_epochs  # Ensure train_dataloader is defined\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcZ4UvY3Skd",
        "outputId": "65f54ca8-debd-4c14-8285-0eaadd094470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Training Loss: 0.3099\n",
            "Validation Accuracy: 0.8775\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.94      0.81      0.87      4961\n",
            "    Positive       0.83      0.95      0.89      5039\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n",
            "Epoch 2/3\n",
            "Training Loss: 0.1691\n",
            "Validation Accuracy: 0.8970\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.88      0.92      0.90      4961\n",
            "    Positive       0.91      0.88      0.90      5039\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "Epoch 3/3\n",
            "Training Loss: 0.0765\n",
            "Validation Accuracy: 0.8983\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.90      0.89      0.90      4961\n",
            "    Positive       0.90      0.90      0.90      5039\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "import torch  # ✅ Ensure PyTorch is imported\n",
        "import torch.nn as nn  # ✅ Import PyTorch's neural network module\n",
        "\n",
        "# ✅ Define the training function\n",
        "def train(model, train_dataloader, optimizer, scheduler, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        # ✅ Ensure loss function is correctly defined\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update weights\n",
        "        scheduler.step()  # Update learning rate\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ✅ Define the evaluation function\n",
        "def evaluate(model, val_dataloader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Convert to numpy array\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "    # ✅ Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    # ✅ Generate classification report\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Negative\", \"Positive\"])\n",
        "\n",
        "    return accuracy, report\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train(model, train_dataloader, optimizer, scheduler, device)  # ✅ Train the model\n",
        "    accuracy, report = evaluate(model, val_dataloader, device)  # ✅ Now this works!\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(report)  # ✅ Print classification report\n",
        "\n",
        "#########################################################################\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "#     train(model, train_dataloader, optimizer, scheduler, device)\n",
        "#     accuracy, report = evaluate(model, val_dataloader, device)\n",
        "#     print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "#     print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yg-tLyiC3Ske"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"bert_classifier.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yxS6T4su3Skf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "IjAc4O153Skf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613ed6d3-bcaf-49a0-e7b5-b79183b19b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie was great and I really enjoyed the performances of the actors.\n",
            "Predicted sentiment: positive\n"
          ]
        }
      ],
      "source": [
        "# Test sentiment prediction\n",
        "test_text = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(\"The movie was great and I really enjoyed the performances of the actors.\")\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "TQZqPCHm3Skg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2d6bf4-e6b1-4cb0-cb0b-55c904349547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst movie of the year.\n",
            "Predicted sentiment: negative\n"
          ]
        }
      ],
      "source": [
        "# Test sentiment prediction\n",
        "test_text = \"Worst movie of the year.\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(\"Worst movie of the year.\")\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vHiE0tny3Skg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691f4f01-af64-43cf-e8a4-d8f9de14d00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-4412236202a3>:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"bert_classifier.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "#just load the modwl and use it for inference\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel  # Make sure you have transformers installed\n",
        "\n",
        "# 1. Define your model class (exactly the same as when you saved it)\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# 2. Create an instance of your model (matching the saved model's architecture)\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "num_classes = 2\n",
        "model = BERTClassifier(bert_model_name, num_classes)\n",
        "\n",
        "# 3. Load the saved state dictionary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #put the device before loading\n",
        "model.to(device) #put the device before loading\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"bert_classifier.pth\", map_location=device))\n",
        "    print(\"Model loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Saved model file not found.\")\n",
        "except RuntimeError as e: # Catch potential size mismatch errors\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# 4. (Optional) Put the model in evaluation mode\n",
        "model.eval()  # Important for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "C2B_LE2E3Skh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32204058-2b19-41ef-a8ef-d6d5b3ebbaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like that.\n",
            "Predicted sentiment: positive\n"
          ]
        }
      ],
      "source": [
        "# Test sentiment prediction\n",
        "test_text = \"i like that.\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(test_text)\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ]
    }
  ]
}